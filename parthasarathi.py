# -*- coding: utf-8 -*-
"""PARTHASARATHI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m6Hf88QBXbECJBhtYlweB-GgYjA2M9n5
"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import pandas as pd
import pandas as pd
import pandas as pd
import pandas as pd

# Load dataset (update filename if needed)
df = pd.read_csv('sarathi.....csv')

# Show first few rows
df.head()
df.head()
df.head()
df.head()

# Shape and structure
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
df.info()
df.describe()

print("Missing Values:\n", df.isnull().sum())
print("Duplicate Rows:", df.duplicated().sum())

import seaborn as sns
import matplotlib.pyplot as plt

# Replace 'target_column' with the actual name of your target column
target_column = df.columns[-1]

sns.histplot(df[target_column], kde=True)
plt.title("Distribution of Target")
plt.xlabel("Target")
plt.show()

target = df.columns[-1]
features = df.columns.drop(target)
print("Features:", features)

categorical_cols = df.select_dtypes(include=['object']).columns
print("Categorical Columns:", categorical_cols.tolist())

df_encoded = pd.get_dummies(df, drop_first=True)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded.drop(target, axis=1))
y = df_encoded[target]

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

from sklearn.impute import SimpleImputer


target_column = df.columns[-1]


imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df_encoded), columns=df_encoded.columns)


X = df_imputed.drop(target_column, axis=1)
y = df_imputed[target_column]


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))

new_sample = df.drop(target, axis=1).iloc[0]
new_df = pd.DataFrame([new_sample])


df_temp = pd.concat([df.drop(target, axis=1), new_df], ignore_index=True)
df_temp_encoded = pd.get_dummies(df_temp, drop_first=True)
df_temp_encoded = df_temp_encoded.reindex(columns=df_encoded.drop(target, axis=1).columns, fill_value=0)


new_input_scaled = scaler.transform(df_temp_encoded.tail(1))
predicted_value = model.predict(new_input_scaled)

print("ðŸŽ¯ Predicted Value:", round(predicted_value[0], 2))

!pip install gradio

import gradio as gr

def predict_from_input(**kwargs):
    input_df = pd.DataFrame([kwargs])
    df_temp = pd.concat([df.drop(target, axis=1), input_df], ignore_index=True)
    df_temp_encoded = pd.get_dummies(df_temp, drop_first=True)
    df_temp_encoded = df_temp_encoded.reindex(columns=df_encoded.drop(target, axis=1).columns, fill_value=0)
    scaled_input = scaler.transform(df_temp_encoded.tail(1))
    prediction = model.predict(scaled_input)
    return round(prediction[0], 2)

# Auto-generate Gradio inputs
inputs = []
for col in df.drop(columns=target).columns:
    if df[col].dtype == 'object':
        inputs.append(gr.Dropdown(choices=df[col].dropna().unique().tolist(), label=col))
    else:
        inputs.append(gr.Number(label=col))

output = gr.Number(label=f"ðŸŽ¯ Predicted {target}")

gr.Interface(fn=predict_from_input, inputs=inputs, outputs=output, title="ðŸ“ˆ Sarathi Prediction App").launch()